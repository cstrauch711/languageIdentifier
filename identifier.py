# -*- coding: utf-8 -*-
"""project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RoWXTSpCMBxq30E0Jn2HiZ_2qCINc4wt

Set up Google Drive to access data.
"""


"""Import Data as Pandas Data Frame"""

import pandas as pd
# Delimeter in CSV files are new line (may be different for mac users)
# UTF-8 encoding is very important as it encompasses all language writing systems, not just latin based ones
# Data was downloaded split into training and testing sets
train = pd.read_csv("data/x_train.txt", header=None, delimiter="\r\n", names=['text'], encoding="utf-8")
train['label'] = pd.read_table("data/y_train.txt", header=None, keep_default_na=False)
test = pd.read_csv("data/x_test.txt", header=None, delimiter="\r\n", names=['text'], encoding="utf-8")
test['label'] = pd.read_table("data/y_test.txt", header=None, keep_default_na=False )
print("train shape:  ", train.shape)
print("test shape:  ", test.shape)

"""A user-defined function to pull the language name (in English) to use as a reference instead of the 3-character language code."""

# Import Langauge Labels as dataframe
langs = pd.read_csv("labels.csv", sep = ';',keep_default_na=False, na_values=[''])

#This function is used if you would like the language name along with the 3-character label
def lang_id(row):
  id = str(row['label'])
  #print("id:  ", id)
  ind = langs.index[langs['Label'] == id].tolist()
  #print("Index:   ", ind)
  return( langs.at[ind[0],'English'] )

# Apply the function above to every row in the data
train['language_name'] = train.apply(lambda row: lang_id(row), axis=1)
test['language_name'] = test.apply(lambda row: lang_id(row), axis=1)

#Data with language names as labels in addition to the language codes for reference reasons (not acual label)
train

"""Data Processing"""

#Remove digits and punctuation (regex) from text strings
train['text'] = train['text'].str.replace('\d+', '')
train['text'] = train['text'].str.replace(r'[^\w\s]+', '')
test['text'] = test['text'].str.replace('\d+', '')
test['text'] = test['text'].str.replace(r'[^\w\s]+', '')

#remove chinese, japanese, and korean characters (2E80â€“9FFF) to reduce chracters used in text
# CJK range: u'[\u4E00-\u9FA5]'
train['text'] = train['text'].str.replace(u'[\u2E80-\u9FA5]', '')

# Only select a certain subset of langauges. These are ones I deemed appropriate for this model because they
#     a) have a significant number of speakers and
#     b) they share a mostly common set of characters (Chinese, Japanese, etc.) were removed because they enlarge the character dictionary too much
include = ['epo', 'ara', 'heb', 'mlt', 'ind', 'vie', 'afr', 'bel', 'bos', 'ces', 'dan', 'deu', 'ell', 'eng', 'fas', 'fra', 'gle', 'hat',
           'hrv', 'hye', 'isl', 'ita', 'lat', 'lav', 'lit', 'nld', 'nno', 'pol', 'por', 'ron', 'rus', 'slk', 'spa', 'sqi', 'srp', 'swe',
           'ukr', 'ibo', 'swa', 'wol', 'xho', 'yor', 'tha', 'aze', 'tur', 'est', 'fin', 'hun', 'msa', 'tgl', 'kur', 'mal', 'tam', 'tel',
           'ben', 'guj', 'hin', 'pan', 'tgk', 'mon', 'kaz', 'tuk', 'uzb']

print("Number of labels:  ", len(include))

#Only include observations of the included languages
train = train[train.label.isin(include)]
test = test[test.label.isin(include)]

print("train shape:  ", train.shape)
print("test shape:  ", test.shape)

#Split training and testing feature (text) from labels (y)
text_train = train['text'].values
y_train = train['label'].values
text_test = test['text'].values
y_test = test['label'].values

print("training feature data shape:  ", text_train.shape)
print("test feature data shape:  ",  text_test.shape)

""""Bag of Words" Approach, but with characters"""

from sklearn.feature_extraction.text import CountVectorizer
#Create vectorizer
vectorizer =  CountVectorizer( lowercase=True, analyzer='char')

#Fit vectorizer to training data
vectorizer.fit(text_train)

#Vectorize the training feature
x_train = vectorizer.transform(text_train).toarray()

#Vectorize the testing feature
x_test = vectorizer.transform(text_test).toarray()

#Look at the different aspects of the vectorizer
print("Training matrix shape", x_train.shape)
print("Vocabulary: ",vectorizer.vocabulary_)
print("Vocabulary words: ",vectorizer.vocabulary_.keys())
print("Vocabulary index: ",vectorizer.vocabulary_.values())
print(x_train)

"""Label Encoding"""

from sklearn.preprocessing import LabelEncoder
# creating instance of labelencoder
labelencoder = LabelEncoder()
# Assigning numerical values and storing in another column
y_train = labelencoder.fit_transform(y_train)
print(y_train)

y_test = labelencoder.transform(y_test)

"""Baseline Logistic Regression Model"""

#2-B: Baseline Logistic Regression Model
from sklearn.linear_model import LogisticRegression
logistic_classifier = LogisticRegression()

logistic_classifier.fit(x_train, y_train)

#2C
score = logistic_classifier.score(x_test, y_test)
print("Accuracy of logistic model:", score)

"""2D:
This model had a fairly good accuracy of 0.92 with a logistic regression.

Deep Neural Network
"""

x_train

x_train.shape

y_train

labelencoder.inverse_transform([0,12,1])

from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras.layers import Dropout, Activation
from keras.layers import BatchNormalization, Dense
from keras.regularizers import l2
from keras import initializers, Sequential
import numpy as np

keras_callbacks   = [
      EarlyStopping(monitor='val_loss', patience=10, mode='min', min_delta=0.0001),
      ModelCheckpoint('./checkmodel.h5', monitor='val_loss', save_best_only=True, mode='min')
]

# Create Sequential model
model = Sequential()
model.add(Dense(500, activation='relu'))
model.add(Dense(500, activation='relu'))
model.add(Dense(500, activation='sigmoid'))
model.add(Dense(250, activation='sigmoid'))
model.add(Dense(63, activation='softmax')) #63 is the number of possible labels

#Sparse categorical crossentropy was chosen because the labels are categorical and in integer format
model.compile(loss = "sparse_categorical_crossentropy", optimizer = "adam", metrics = ['accuracy'])

train_history = model.fit(x_train, y_train, validation_split=0.1, batch_size = 5, epochs = 30, callbacks=keras_callbacks)

#Testing Accuracy
print("Accuracy of neural network model:", model.evaluate(x_test, y_test)[1])

"""Accuracy and Loss Visualizations"""

import matplotlib.pyplot as plt
plt.figure(figsize=(12,4))

plt.subplot(1,2,1)
# Plot training & validation loss values
plt.plot(train_history.history['loss'], label='Train')
plt.plot(train_history.history['val_loss'], label='Validation')
plt.title('Training & Validation Loss', fontsize=15)
plt.ylabel('Loss', fontsize=15)
plt.xlabel('Epoch', fontsize=15)
plt.xticks( fontsize=12)
plt.yticks( fontsize=12)
plt.legend(loc='upper right', fontsize=12)


plt.subplot(1,2,2)
# Plot training & validation accuracy values
plt.plot(train_history.history['accuracy'], label='Train')
plt.plot(train_history.history['val_accuracy'], label='Validation')
plt.title('Training & Validation accuracy', fontsize=15)
plt.ylabel('accuracy', fontsize=15)
plt.xlabel('Epoch', fontsize=15)
plt.xticks( fontsize=12)
plt.yticks( fontsize=12)
plt.legend(loc='lower right', fontsize=12)

plt.tight_layout()
plt.show()
